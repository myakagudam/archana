{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP3PoVpvg9FV92YtzJc0xRg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myakagudam/archana/blob/main/Copy_of_Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1. Create DataFrame from Python List of Tuples (Hardcoded data)\n",
        "Scenario: For quick unit testing or mocking input data before pipeline execution.\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DataFrame Creation\").getOrCreate()\n",
        "\n",
        "data = [(1, \"Alice\", 5000), (2, \"Bob\", 6000)]\n",
        "columns = [\"id\", \"name\", \"salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "uDlRD0VKjYha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8VHiz99HiwJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "2. Create DataFrame from CSV File (Structured File Input)\n",
        "Scenario: Loading transaction logs from SFTP/Blob/ADLS in daily batch jobs.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "df = spark.read.option(\"header\", True).csv(\"/mnt/raw/transactions.csv\")\n",
        "df.printSchema()\n",
        "df.show(5)\n"
      ],
      "metadata": {
        "id": "VIChQZvHkmDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3. Create DataFrame from JSON File\n",
        "Scenario: Ingesting nested event data from application APIs.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "df = spark.read.option(\"multiline\", True).json(\"/mnt/raw/events.json\")\n",
        "df.select(\"eventType\", \"timestamp\").show()"
      ],
      "metadata": {
        "id": "5yIQpTR1m2-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " 4. Create DataFrame from Parquet File\n",
        "Scenario: Standard optimized input/output in data lake pipelines.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "df = spark.read.parquet(\"/mnt/silver/user_profile\")\n",
        "df.createOrReplaceTempView(\"user_profile\")\n"
      ],
      "metadata": {
        "id": "mxkGlFG9lhKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5. Create DataFrame from SQL Table (JDBC)\n",
        "Scenario: Read from MySQL, PostgreSQL, or SQL Server using ADF or directly in Databricks.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "jdbc_url = \"jdbc:mysql://hostname:3306/dbname\"\n",
        "df = spark.read \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", jdbc_url) \\\n",
        "    .option(\"dbtable\", \"transactions\") \\\n",
        "    .option(\"user\", \"username\") \\\n",
        "    .option(\"password\", \"password\") \\\n",
        "    .load()\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "jmDsWxVklmy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " 6. Create DataFrame from Delta Table\n",
        "Scenario: Reading data from Bronze/Silver/Gold layers in the Medallion Architecture.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "df = spark.read.format(\"delta\").load(\"/mnt/gold/suspicious_transactions\")\n",
        "df.filter(\"amount > 10000\").show()"
      ],
      "metadata": {
        "id": "ZuU-d6Dvlfyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " 7. Create DataFrame using RDD\n",
        "Scenario: Legacy Spark or custom transformation use case.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "rdd = spark.sparkContext.parallelize([(1, \"Alice\"), (2, \"Bob\")])\n",
        "df = rdd.toDF([\"id\", \"name\"])\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "IJ5rf8c5lxmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8. Create DataFrame with Schema Definition (StructType)\n",
        "Scenario: Ensuring strict schema validation on ingestion.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"salary\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "data = [(1, \"Alice\", 5000), (2, \"Bob\", 6000)]\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "id": "TKh5pRyClx_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9. Create DataFrame from Streaming Source (Kafka)\n",
        "Scenario: Real-time ingestion of AML events or payment messages.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "df = spark.readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"broker:9092\") \\\n",
        "    .option(\"subscribe\", \"transactions\") \\\n",
        "    .load()"
      ],
      "metadata": {
        "id": "hyeUmfpBl8sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Hardcoded Data (List of Tuples)\n",
        "Question:\n",
        "You're designing a unit test for a transformation logic. How would you create a DataFrame with test data?\n",
        "âœ… Tip: Use createDataFrame(data, schema) to simulate source data.\n",
        "\n",
        "ðŸ”¹ 2. CSV File\n",
        "Question:\n",
        "Your daily batch job loads transaction data from a CSV file on ADLS. How will you ensure schema consistency and skip corrupt records?\n",
        "âœ… Tip: Use .option(\"header\", True).option(\"mode\", \"DROPMALFORMED\")\n",
        "\n",
        "ðŸ”¹ 3. JSON File (Nested Structure)\n",
        "Question:\n",
        "How do you read and flatten a nested JSON file received from a third-party API?\n",
        "âœ… Tip: Use df.selectExpr(\"payload.id\", \"payload.name\") or explode() for arrays.\n",
        "\n",
        "ðŸ”¹ 4. Parquet File\n",
        "Question:\n",
        "Why would you choose Parquet format over CSV in your Gold layer?\n",
        "âœ… Tip: Explain about columnar storage, faster queries, and compression benefits.\n",
        "\n",
        "ðŸ”¹ 5. JDBC Table\n",
        "Question:\n",
        "How would you pull 1 million records from a MySQL database into Spark efficiently?\n",
        "âœ… Tip: Use .option(\"partitionColumn\", \"id\") with .option(\"numPartitions\", 4).\n",
        "\n",
        "ðŸ”¹ 6. Delta Table\n",
        "Question:\n",
        "Your Gold layer stores high-value AML alerts. How do you query the latest snapshot with fast performance?\n",
        "âœ… Tip: Delta format + Z-Order by transaction_date.\n",
        "\n",
        "ðŸ”¹ 7. RDD Conversion\n",
        "Question:\n",
        "You have an RDD from a legacy Spark job. How do you convert it to a DataFrame with schema?\n",
        "âœ… Tip: Use rdd.toDF([\"col1\", \"col2\"]).\n",
        "\n",
        "ðŸ”¹ 8. With Defined Schema\n",
        "Question:\n",
        "Why is it important to define a schema when reading raw ingestion files?\n",
        "âœ… Tip: Prevents schema inference errors, ensures type consistency.\n",
        "\n",
        "ðŸ”¹ 9. Kafka Stream\n",
        "Question:\n",
        "How do you read real-time transaction messages from a Kafka topic and convert the value to string?\n",
        "âœ… Tip:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "df.selectExpr(\"CAST(value AS STRING)\")\n",
        "\n",
        "Question:\n",
        "You receive CSV, JSON, and Parquet files from different sources every day. How do you handle schema differences and create a unified DataFrame?\n",
        "âœ… Tip: Use schema merging with .option(\"mergeSchema\", \"true\") for Parquet, or define StructType and standardize before union."
      ],
      "metadata": {
        "id": "secykC48mM7M"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hpmZ9hZUmL8g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}